{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdVjz9pzQKl9kIwrNlsKBR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sans-codes/2462364_SanskritiAryal/blob/main/Worksheet6_SanskritiAryal_5CS037_ConceptsAndTechnologiesOfAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Sigmoid Function:"
      ],
      "metadata": {
        "id": "DPxyx0i2ZffE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_function(x):\n",
        "    \"\"\"\n",
        "    Computes the logistic function applied to any value of x.\n",
        "    Arguments:\n",
        "    x: scalar or numpy array of any size.\n",
        "    Returns:\n",
        "    y: logistic function applied to x.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y"
      ],
      "metadata": {
        "id": "U1iLl4OIFNka"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Case for logistic function:"
      ],
      "metadata": {
        "id": "h40PDuNKFbKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def test_logistic_function():\n",
        "    \"\"\"\n",
        "    Test cases for the logistic_function.\n",
        "    \"\"\"\n",
        "    x_scalar = 0\n",
        "    expected_output_scalar = round(1 / (1 + np.exp(0)), 3)\n",
        "    assert round(logistic_function(x_scalar), 3) == expected_output_scalar\n",
        "\n",
        "    x_pos = 2\n",
        "    expected_output_pos = round(1 / (1 + np.exp(-2)), 3)\n",
        "    assert round(logistic_function(x_pos), 3) == expected_output_pos\n",
        "\n",
        "    x_neg = -3\n",
        "    expected_output_neg = round(1 / (1 + np.exp(3)), 3)\n",
        "    assert round(logistic_function(x_neg), 3) == expected_output_neg\n",
        "\n",
        "    x_array = np.array([0, 2, -3])\n",
        "    expected_output_array = np.array([0.5, 0.881, 0.047])\n",
        "    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array)\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_logistic_function()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEaabnsmFcfm",
        "outputId": "6b7e44d0-eaa8-455b-ed6f-7d6b3497ab69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of log - loss function:"
      ],
      "metadata": {
        "id": "qLUPXZTfGNPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for true target value y ={0 or 1} and predicted target value yâ€™ inbetween {0-1}.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "hPVKlMlxGODi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intuition verification"
      ],
      "metadata": {
        "id": "OmfyzuM_GZbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred = 0, 0.1\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
        "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
        "y_true, y_pred = 1, 0.9\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n"
      ],
      "metadata": {
        "id": "zvNRI0E9Gazz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634f6f2b-5124-4bef-9c00-1eed229cf56b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log loss(0, 0.1) ==> 0.10536051565782628\n",
            "+++++++++++++--------------------------++++++++++++++++++++++++\n",
            "log loss(1, 0.9) ==> 0.10536051565782628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test case"
      ],
      "metadata": {
        "id": "1b6X0ggEGgA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_log_loss():\n",
        "    import numpy as np\n",
        "    y_true, y_pred = 1, 1\n",
        "    assert np.isclose(log_loss(y_true, y_pred), 0.0)\n",
        "\n",
        "    y_true, y_pred = 0, 0\n",
        "    assert np.isclose(log_loss(y_true, y_pred), 0.0)\n",
        "\n",
        "    y_true, y_pred = 1, 0.8\n",
        "    expected_loss = -(np.log(0.8))\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6)\n",
        "\n",
        "    y_true, y_pred = 0, 0.2\n",
        "    expected_loss = -(np.log(0.8))\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6)\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_log_loss()"
      ],
      "metadata": {
        "id": "AoLaZ_w2GhOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e10c27-98af-47f9-fc65-8f698e5b6d88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost Function"
      ],
      "metadata": {
        "id": "TqKzn_haGpn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    assert len(y_true) == len(y_pred)\n",
        "    n = len(y_true)\n",
        "    loss_vec = log_loss(y_true, y_pred)\n",
        "    cost = np.sum(loss_vec) / n\n",
        "    return cost"
      ],
      "metadata": {
        "id": "zvKqa8-EGqdq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "FO1voCjTGviA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def test_cost_function():\n",
        "    y_true = np.array([1, 0, 1])\n",
        "    y_pred = np.array([0.9, 0.1, 0.8])\n",
        "\n",
        "    expected_cost = (-(np.log(0.9)) - np.log(0.9) - np.log(0.8)) / 3\n",
        "    result = cost_function(y_true, y_pred)\n",
        "\n",
        "    assert np.isclose(result, expected_cost, atol=1e-6)\n",
        "    print(\"Test passed!\")\n",
        "\n",
        "test_cost_function()"
      ],
      "metadata": {
        "id": "kFIQumy6GyKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d13456-0e66-4eda-9111-2978d69d8f91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorized Cost Function"
      ],
      "metadata": {
        "id": "OjFLZRI6G1qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def costfunction_logreg(X, y, w, b):\n",
        "    import numpy as np\n",
        "    n, d = X.shape\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "    cost = cost_function(y, y_pred)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "_BDQ0g2xG5wl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Gradient"
      ],
      "metadata": {
        "id": "L2jrimG0G8Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "    import numpy as np\n",
        "    n, d = X.shape\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "\n",
        "    grad_w = np.dot(X.T, (y_pred - y)) / n\n",
        "    grad_b = np.sum(y_pred - y) / n\n",
        "\n",
        "    return grad_w, grad_b"
      ],
      "metadata": {
        "id": "F3qhyzbAHFZj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent"
      ],
      "metadata": {
        "id": "oxpv7vWoHKJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "    n, d = X.shape\n",
        "    cost_history = []\n",
        "    params_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "        w -= alpha * grad_w\n",
        "        b -= alpha * grad_b\n",
        "\n",
        "        cost = costfunction_logreg(X, y, w, b)\n",
        "        cost_history.append(cost)\n",
        "        params_history.append((w.copy(), b))\n",
        "\n",
        "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "\n",
        "    return w, b, cost_history, params_history"
      ],
      "metadata": {
        "id": "XEvT-bH_HK79"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction Function"
      ],
      "metadata": {
        "id": "A24xa_BHHNqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(X, w, b, threshold=0.5):\n",
        "    import numpy as np\n",
        "    z = np.dot(X, w) + b\n",
        "    y_test_prob = logistic_function(z)\n",
        "    y_pred = (y_test_prob >= threshold).astype(int)\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "aKpNZEYYHQVu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Function"
      ],
      "metadata": {
        "id": "NBzQHRLmHTgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(y_true, y_pred):\n",
        "    import numpy as np\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"confusion_matrix\": np.array([[TN, FP], [FN, TP]]),\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score\n",
        "    }"
      ],
      "metadata": {
        "id": "7MKs5m-GHWEN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART II SOFTMAX REGRESSION"
      ],
      "metadata": {
        "id": "l7-FmXeIHYp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Softmax Function"
      ],
      "metadata": {
        "id": "-xc58zS2em9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Compute the softmax of a 2D numpy array along the specified axis.\n",
        "    \"\"\"\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "vSxLgIHXentY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.array([[1, 2, 3]])\n",
        "y = softmax(z)\n",
        "\n",
        "print(\"Softmax output:\", y)\n",
        "print(\"Sum of probabilities:\", np.sum(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7k4LNAofP8q",
        "outputId": "f0840d0e-16fb-4312-bd25-6616f82ccb27"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax output: [[0.09003057 0.24472847 0.66524096]]\n",
            "Sum of probabilities: 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Loss Function"
      ],
      "metadata": {
        "id": "AC42sMUgfUwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_softmax(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-10))"
      ],
      "metadata": {
        "id": "6TDxP3j2fV1P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([[0, 0, 1]])\n",
        "y_pred = np.array([[0.1, 0.2, 0.7]])\n",
        "print(\"Softmax Loss:\", loss_softmax(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-b77h2DfZTg",
        "outputId": "a055f7ab-b2fc-4a7a-a56e-f867174c4246"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Loss: 0.3566749437958753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Cost Function"
      ],
      "metadata": {
        "id": "X5uqfIVefcG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_softmax(X, y, W, b):\n",
        "    n, d = X.shape\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "    return -np.sum(y * np.log(y_pred + 1e-10)) / n"
      ],
      "metadata": {
        "id": "XyhvMVkEfeVX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 2]])\n",
        "y = np.array([[0, 1]])\n",
        "W = np.array([[0.1, 0.2],\n",
        "              [0.3, 0.4]])\n",
        "b = np.array([0.1, 0.1])\n",
        "\n",
        "print(\"Softmax Cost:\", cost_softmax(X, y, W, b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSYGchuufgPu",
        "outputId": "0477b00f-13b7-40c4-a741-1bfc80e9de0f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Cost: 0.5543552442944452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART II VERIFICATION:\n",
        "The softmax function was verified using a sample input.\n",
        "The output probabilities are between 0 and 1, and their sum equals 1.\n",
        "The loss and cost functions produce valid numerical outputs, confirming correct implementation.\n"
      ],
      "metadata": {
        "id": "G7sw_WbjfmIs"
      }
    }
  ]
}